{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNH5Cd1VhkHo15F7VT3chjQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlsgptj/ingongginung_2/blob/main/%EC%9D%B8%EA%B3%B5%EB%8A%A5%EC%A7%80_%EA%B8%B0%EB%A7%90%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(HW#2) 교재 13장 6절의 다층퍼셉트론 프로그램을 이용하여 UCI의 iris data(붓꽃데이터)를 분류하는 신경망을 완성하시오.\n",
        "\n",
        "★ 과제 1처럼 PDF 보고서 파일과 소스코드 파일을 제출해 주세요! (양식이 잘못된 경우 감점될 수 있습니다)\n",
        "\n",
        "★ 시스템상 파일 1개만 업로드되므로, 압축(zip)하여 올려주세요!\n",
        "\n",
        "★ 평가 기준 등은 과제 2 설명 동영상을 참고해 주세요!\n",
        "\n",
        "★ 소스코드 파일(mlp.py)로부터 시작하면 조금 더 편리합니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- 입력층 노드수: 4\n",
        "\n",
        "- 은닉층 노드수: 5\n",
        "\n",
        "- 출력층 노드수: 3\n",
        "\n",
        "\n",
        "\n",
        "(1) sklearn 라이브러리 활용하여 붓꽃데이터 읽어들임 - datasets.load_iris()\n",
        "\n",
        "(2) iris data에는 붓꽃샘플이 3가지 클래스 각각에 대해 50개씩, 총 150개의 샘플이 포함되어 있음\n",
        "\n",
        "(3) 각각의 샘플은 4개의 실수값으로 구성된 4차원 벡터\n",
        "\n",
        "(4) iris data를 훈련용 120개, 테스트용 30개로 구분하여 사용하고, 훈련데이터에 대한 학습결과와 테스트용 데이터 30개에 대한 분류성능 측정\n",
        "\n",
        "(5) 신경망의 은닉층 및 출력층 노드에서는 바이어스(bias)를 사용하고, 활성화함수로 sigmoid 함수를 사용\n",
        "\n",
        "(6) BP알고리즘에서 최대 epoch는 50회, 학습율은 0.01을 사용\n",
        "\n",
        "(7) 다층퍼셉트론 신경망에 대한 훈련의 결과로 얻어진 모든 weight와 bias 값을 출력 (random하게 초기화)\n",
        "\n",
        "(8) Python 소스코드와 학습결과에 대한 분석을 정리하여 리포트로 제출"
      ],
      "metadata": {
        "id": "Jhj9dlES2Bub"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLUa64JQ1V8s"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#붓꽃 데이터 로드\n",
        "iris = load_iris()\n",
        "print(iris)"
      ],
      "metadata": {
        "id": "0Fyb-SB-2AtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_df = pd.DataFrame(data=iris.data, columns = iris.feature_names)\n",
        "iris_df['species'] = iris.target\n",
        "\n",
        "print(iris_df.tail(50))\n",
        "#품종 전처리는 이미 되어있는 것 같음 0, 1, 2로"
      ],
      "metadata": {
        "id": "ruicwags27Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris.data\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "N6NX8Zod6G_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "nBRAOPAf5dvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Id2JnW9x58H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_deriv(out):\n",
        "  return out*(1-out)\n",
        "\n",
        "#출력층 활성화 함수(softmax)\n",
        "def softmax(x):\n",
        "  #안정적인 softmax 구현\n",
        "  c = np.max(x, axis=1,keepdims = True)\n",
        "  exp_x = np.exp(x-c)\n",
        "  return exp_x / np.sum(exp_x, axis=1, keepdims =True)\n",
        "\n",
        "#Cross-entropy Loss\n",
        "def cross_entropy(pred, target):\n",
        "    # pred: N x 3, target: N x 3 (one-hot)\n",
        "    # log(0)이 발생하지 않게 작은 값을 더해줌\n",
        "    return -np.sum(target * np.log(pred + 1e-9)) / len(target)\n",
        "\n",
        "inputs = 4\n",
        "hiddens = 5\n",
        "outputs = 3\n",
        "\n",
        "learning_rate = 0.05\n",
        "\n",
        "# 가중치 초기화\n",
        "np.random.seed(0)\n",
        "W1 = np.random.randn(inputs, hiddens)*0.01\n",
        "B1 = np.zeros((hiddens,))\n",
        "W2 = np.random.randn(hiddens, outputs)*0.01\n",
        "B2 = np.zeros((outputs,))\n",
        ""
      ],
      "metadata": {
        "id": "1VIo_dYR3QFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x):\n",
        "    # 순방향 전파\n",
        "    layer0 = x  # (N, 4)\n",
        "    Z1 = np.dot(layer0, W1) + B1  # (N, 5)\n",
        "    layer1 = sigmoid(Z1)          # (N, 5)\n",
        "    Z2 = np.dot(layer1, W2) + B2  # (N, 3)\n",
        "    layer2 = softmax(Z2)          # (N, 3)\n",
        "    return layer0, layer1, layer2"
      ],
      "metadata": {
        "id": "2LTy6HGs31cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def fit(epochs=3000):\n",
        "    global W1, W2, B1, B2\n",
        "\n",
        "    # Create a OneHotEncoder instance outside the loop\n",
        "    encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "    # Fit the encoder to your training labels\n",
        "    Y_train_onehot = encoder.fit_transform(Y_train.reshape(-1, 1))\n",
        "\n",
        "    for i in range(epochs):\n",
        "        layer0, layer1, layer2 = predict(X_train)\n",
        "\n",
        "        # 오차 계산 (cross-entropy)\n",
        "        loss = cross_entropy(layer2, Y_train)\n",
        "\n",
        "        # 역전파\n",
        "        # layer2_delta: 출력층 gradient (softmax + cross-entropy에서 pred - target)\n",
        "        # Use the one-hot encoded Y_train here\n",
        "        layer2_delta = (layer2 - Y_train_onehot) / len(X_train)  # (N, 3)\n",
        "\n",
        "        # 은닉층 오차\n",
        "        layer1_error = np.dot(layer2_delta, W2.T) # (N,5)\n",
        "        layer1_delta = layer1_error * sigmoid_deriv(layer1) # (N,5)\n",
        "\n",
        "        # 가중치 업데이트\n",
        "        W2 -= learning_rate * np.dot(layer1.T, layer2_delta) # (5,3)\n",
        "        B2 -= learning_rate * np.sum(layer2_delta, axis=0)   # (3,)\n",
        "        W1 -= learning_rate * np.dot(layer0.T, layer1_delta) # (4,5)\n",
        "        B1 -= learning_rate * np.sum(layer1_delta, axis=0)   # (5,)\n",
        "\n",
        "        if (i+1) % 500 == 0:\n",
        "            print(f\"Epoch {i+1}/{epochs}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "_rCU4dqGbZg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(X_data, Y_data):\n",
        "    _, _, layer2 = predict(X_data)\n",
        "    pred_classes = np.argmax(layer2, axis=1)\n",
        "    # Change axis to 0 to find the maximum along the first axis (rows)\n",
        "    true_classes = np.argmax(Y_data, axis=0)\n",
        "    accuracy = np.mean(pred_classes == true_classes)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "lEsWs-ANbdlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "__kQxYgTbwwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def cross_entropy(pred, target):\n",
        "    # Convert target to one-hot encoding\n",
        "    encoder = OneHotEncoder(sparse_output=False, categories='auto') # 'auto' to infer categories from data\n",
        "    target_onehot = encoder.fit_transform(target.reshape(-1, 1)) # Reshape target to 2D for OneHotEncoder\n",
        "\n",
        "    # pred: N x 3, target: N x 3 (one-hot)\n",
        "    # log(0)이 발생하지 않게 작은 값을 더해줌\n",
        "    return -np.sum(target_onehot * np.log(pred + 1e-9)) / len(target)"
      ],
      "metadata": {
        "id": "Sqze4AbEbjBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(epochs=5000)"
      ],
      "metadata": {
        "id": "VRrFovAybkd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc = evaluate(X_train, Y_train)\n",
        "test_acc = evaluate(X_test, Y_test)\n",
        "print(f\"Train Accuracy: {train_acc*100:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "Wo5zyQ_dcBHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TrDDdo79cVMp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}