{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMPTYBa09gYOsZrN/32nWfz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlsgptj/ingongginung_2/blob/main/%EC%9D%B8%EA%B3%B5%EB%8A%A5%EC%A7%80_%EA%B8%B0%EB%A7%90%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(HW#2) 교재 13장 6절의 다층퍼셉트론 프로그램을 이용하여 UCI의 iris data(붓꽃데이터)를 분류하는 신경망을 완성하시오.\n",
        "\n",
        "★ 과제 1처럼 PDF 보고서 파일과 소스코드 파일을 제출해 주세요! (양식이 잘못된 경우 감점될 수 있습니다)\n",
        "\n",
        "★ 시스템상 파일 1개만 업로드되므로, 압축(zip)하여 올려주세요!\n",
        "\n",
        "★ 평가 기준 등은 과제 2 설명 동영상을 참고해 주세요!\n",
        "\n",
        "★ 소스코드 파일(mlp.py)로부터 시작하면 조금 더 편리합니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- 입력층 노드수: 4\n",
        "\n",
        "- 은닉층 노드수: 5\n",
        "\n",
        "- 출력층 노드수: 3\n",
        "\n",
        "\n",
        "\n",
        "(1) sklearn 라이브러리 활용하여 붓꽃데이터 읽어들임 - datasets.load_iris()\n",
        "\n",
        "(2) iris data에는 붓꽃샘플이 3가지 클래스 각각에 대해 50개씩, 총 150개의 샘플이 포함되어 있음\n",
        "\n",
        "(3) 각각의 샘플은 4개의 실수값으로 구성된 4차원 벡터\n",
        "\n",
        "(4) iris data를 훈련용 120개, 테스트용 30개로 구분하여 사용하고, 훈련데이터에 대한 학습결과와 테스트용 데이터 30개에 대한 분류성능 측정\n",
        "\n",
        "(5) 신경망의 은닉층 및 출력층 노드에서는 바이어스(bias)를 사용하고, 활성화함수로 sigmoid 함수를 사용\n",
        "\n",
        "(6) BP알고리즘에서 최대 epoch는 50회, 학습율은 0.01을 사용\n",
        "\n",
        "(7) 다층퍼셉트론 신경망에 대한 훈련의 결과로 얻어진 모든 weight와 bias 값을 출력 (random하게 초기화)\n",
        "\n",
        "(8) Python 소스코드와 학습결과에 대한 분석을 정리하여 리포트로 제출"
      ],
      "metadata": {
        "id": "Jhj9dlES2Bub"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLUa64JQ1V8s"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 로드"
      ],
      "metadata": {
        "id": "S4URII_oo9FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#붓꽃 데이터 로드\n",
        "iris = load_iris()\n",
        "print(iris)"
      ],
      "metadata": {
        "id": "0Fyb-SB-2AtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 프레임"
      ],
      "metadata": {
        "id": "M_chOJ7qo_hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris_df = pd.DataFrame(data=iris.data, columns = iris.feature_names)\n",
        "iris_df['species'] = iris.target\n",
        "\n",
        "print(iris_df.tail(50))"
      ],
      "metadata": {
        "id": "ruicwags27Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#key값 확인\n",
        "iris.keys()"
      ],
      "metadata": {
        "id": "zc6D3b8ApXAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module' 6가지의 키값을 가지고 있음"
      ],
      "metadata": {
        "id": "rHxKWPkMpeT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 셋에 대한 설명을 볼 수 있음\n",
        "print(iris.DESCR)"
      ],
      "metadata": {
        "id": "SBwA2YjVpnKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "150개의 붓꽃 샘플의 데이터가 정리되어 있음\n",
        "Attribute Information에서 4개의 피쳐가 있음\n",
        "-> sepal, petal, 가로길이, 세로길이\n",
        "-> class에서 setosa, versicolur, virginica 3가지 품종이 있음을 파악함"
      ],
      "metadata": {
        "id": "dHsU0vgXp0oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris_df.describe()"
      ],
      "metadata": {
        "id": "TM_BHwLEqSKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_df.isnull().sum()\n",
        "#결측값 파악"
      ],
      "metadata": {
        "id": "V0H7QfOfqloe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_df.duplicated().sum()\n",
        "#중복값이 있는지 파악함 1개의 중복값 존재\n",
        "#중복값은 제거해주는 것이 좋음"
      ],
      "metadata": {
        "id": "lSeltLalqsIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = iris_df.drop_duplicates()\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "chDSTNMYq9Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이번엔 상관관계를 분석해보기로 함\n",
        "df.corr()"
      ],
      "metadata": {
        "id": "1mWK2xuPrH6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#변수 간 상관관계 분석(히트맵)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PXgaiwDFrOo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[히트맵 분석]\n",
        "1. 대각선은 항상 1임 자신과의 상관관계\n",
        "2. 1에 가까울수록 강한 양의 상관관계\n",
        "3. -1에 가까울수록 강한 음의 상관관계\n",
        "4. 값이 0에 가까울수록 상관관계가 적음\n",
        "5. species와 petal length : 0.95\n",
        "6. species와 petal width : 0.96\n",
        "7. 이 두가지 요소는 종 분류에 기여할 가능성이 높음\n",
        "8. Sepal Width는 상관계수가 전반적으로 낮아 주요 변수로 보기에 어려움이 있음"
      ],
      "metadata": {
        "id": "1KDklbU-sFgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(df['sepal length (cm)'], bins=10, edgecolor='black')\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Sepal Length')\n",
        "plt.show()\n",
        "# sepal length를 분석"
      ],
      "metadata": {
        "id": "kb9Flet0tF_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(df['petal length (cm)'], bins=10, edgecolor='black')\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Sepal Length')\n",
        "plt.show()\n",
        "# petal length를 분석"
      ],
      "metadata": {
        "id": "6IuZBqyrvW-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(df['petal width (cm)'], bins=10, edgecolor='black')\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Sepal Length')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r4EseJkMvg0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#품종에 따라 분류\n",
        "species_0 = df[df['species'] == 0]\n",
        "species_1 = df[df['species'] == 1]\n",
        "species_2 = df[df['species'] == 2]"
      ],
      "metadata": {
        "id": "lwU5ID-DwI3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(\n",
        "    x=\"sepal length (cm)\",\n",
        "    hue=\"species\",\n",
        "    kind=\"kde\",\n",
        "    data=df,\n",
        "    fill=True\n",
        ")"
      ],
      "metadata": {
        "id": "6ai17P2SxPnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[그래프 해석]\n",
        "1. 종류0 -> sepal length가 상대적으로 짧고, 값의 변동이 적은 특징을 보임\n",
        "2. 종류1 -> 중간정도, 변동 폭도 중간\n",
        "3. 종류2 -> 길고 변동이 큼"
      ],
      "metadata": {
        "id": "GNEie5WJxfyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(\n",
        "    x=\"petal length (cm)\",\n",
        "    hue=\"species\",\n",
        "    kind=\"kde\",\n",
        "    data=df,\n",
        "    fill=True\n",
        ")"
      ],
      "metadata": {
        "id": "fnRR5RHNx4H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "붓꽃의 품종을 분류하는 데 중요한 변수임, 특히 종류 0을 구분하는데 용이"
      ],
      "metadata": {
        "id": "pm7-dJDHyQXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(\n",
        "    x=\"sepal width (cm)\",\n",
        "    hue=\"species\",\n",
        "    kind=\"kde\",\n",
        "    data=df,\n",
        "    fill=True\n",
        ")"
      ],
      "metadata": {
        "id": "BBs5ckU_yYuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(\n",
        "    x=\"petal width (cm)\",\n",
        "    hue=\"species\",\n",
        "    kind=\"kde\",\n",
        "    data=df,\n",
        "    fill=True\n",
        ")"
      ],
      "metadata": {
        "id": "vmUU77k3y92S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 품종0은 꽃잎 너비가 작아 다른 품종과 구분됨(분류하기 비교적 쉬움)\n",
        "2. 부분적으로 겹쳐있지만, 중심의 차이를 통해 어느정도 구분 가능\n",
        "3. 데이터 분석을 통해 Sepal Width의 그래프 빼고는 어느정도 비슷한 형태로 그래프가 도출됨"
      ],
      "metadata": {
        "id": "R6Za18g4yj7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습"
      ],
      "metadata": {
        "id": "Xa5RYYDSo2_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Sepal Width의 변수를 포함하고 모델을 학습시킨 결과"
      ],
      "metadata": {
        "id": "lcoNBBG70YSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nBRAOPAf5dvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#특성과 타겟 분리\n",
        "X = df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']].values\n",
        "y = df['species'].values"
      ],
      "metadata": {
        "id": "bx4SaLQl0p9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 표준화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "caXtU-uEMIR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Id2JnW9x58H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 원-핫 인코딩\n",
        "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "Y_train_onehot = encoder.fit_transform(Y_train.reshape(-1, 1))\n",
        "Y_test_onehot = encoder.transform(Y_test.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "OLkcg7zhKe1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "inputs = 4  # 입력층 노드 수\n",
        "hiddens = 5  # 은닉층 노드 수\n",
        "outputs = 3  # 출력층 노드 수\n",
        "learning_rate = 0.01  # 학습률\n",
        "epochs = 50  # 최대 학습 에포크 수"
      ],
      "metadata": {
        "id": "dL7PkC_WKiIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 및 바이어스 초기화\n",
        "np.random.seed(42)  # 재현성을 위해 시드 설정\n",
        "W1 = np.random.randn(inputs, hiddens) * 0.01  # 입력층 -> 은닉층 가중치\n",
        "B1 = np.zeros((hiddens,))  # 은닉층 바이어스\n",
        "W2 = np.random.randn(hiddens, outputs) * 0.01  # 은닉층 -> 출력층 가중치\n",
        "B2 = np.zeros((outputs,))  # 출력층 바이어스"
      ],
      "metadata": {
        "id": "_MaggdkUKj9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 활성화 함수: sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# sigmoid 함수의 미분\n",
        "def sigmoid_deriv(out):\n",
        "    return out * (1 - out)\n",
        "\n",
        "# 출력층 활성화 함수: softmax\n",
        "def softmax(x):\n",
        "    c = np.max(x, axis=1, keepdims=True)  # 안정적인 softmax 구현\n",
        "    exp_x = np.exp(x - c)\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# 손실 함수: cross-entropy\n",
        "def cross_entropy(pred, target):\n",
        "    return -np.sum(target * np.log(pred + 1e-9)) / len(target)\n",
        "\n",
        "# 모델의 순방향 전파\n",
        "def predict(x):\n",
        "    if x.shape[1] != W1.shape[0]:\n",
        "        raise ValueError(f\"Input shape mismatch: expected {W1.shape[0]}, got {x.shape[1]}\")\n",
        "    layer0 = x  # 입력층 (N, 4)\n",
        "    Z1 = np.dot(layer0, W1) + B1  # 은닉층 선형 연산 (N, 5)\n",
        "    layer1 = sigmoid(Z1)  # 은닉층 활성화 함수 적용 (N, 5)\n",
        "    Z2 = np.dot(layer1, W2) + B2  # 출력층 선형 연산 (N, 3)\n",
        "    layer2 = softmax(Z2)  # 출력층 활성화 함수 적용 (N, 3)\n",
        "    return layer0, layer1, layer2"
      ],
      "metadata": {
        "id": "9kR1073IKmDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "def fit(epochs):\n",
        "    global W1, W2, B1, B2\n",
        "\n",
        "    for i in range(epochs):  # 최대 epochs 반복\n",
        "        # 순방향 전파\n",
        "        layer0, layer1, layer2 = predict(X_train)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = cross_entropy(layer2, Y_train_onehot)\n",
        "\n",
        "        # 역전파\n",
        "        layer2_delta = (layer2 - Y_train_onehot) / len(X_train)  # 출력층 gradient\n",
        "        layer1_error = np.dot(layer2_delta, W2.T)  # 은닉층 오차\n",
        "        layer1_delta = layer1_error * sigmoid_deriv(layer1)  # 은닉층 gradient\n",
        "\n",
        "        # 가중치 및 바이어스 업데이트\n",
        "        W2 -= learning_rate * np.dot(layer1.T, layer2_delta)  # 은닉층 -> 출력층 가중치 업데이트\n",
        "        B2 -= learning_rate * np.sum(layer2_delta, axis=0)  # 출력층 바이어스 업데이트\n",
        "        W1 -= learning_rate * np.dot(layer0.T, layer1_delta)  # 입력층 -> 은닉층 가중치 업데이트\n",
        "        B1 -= learning_rate * np.sum(layer1_delta, axis=0)  # 은닉층 바이어스 업데이트\n",
        "\n",
        "        # 학습 상태 출력\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch {i + 1}/{epochs}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "pW_skuulKols"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "def evaluate(X_data, Y_data):\n",
        "    _, _, layer2 = predict(X_data)\n",
        "    pred_classes = np.argmax(layer2, axis=1)  # 예측 클래스\n",
        "    true_classes = np.argmax(Y_data, axis=1)  # 실제 클래스\n",
        "    accuracy = np.mean(pred_classes == true_classes)  # 정확도 계산\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "Pk_lCDzvKrJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 실행\n",
        "fit(epochs=50)\n",
        "\n",
        "# 테스트 데이터 평가\n",
        "accuracy = evaluate(X_test, Y_test_onehot)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# 가중치 및 바이어스 출력\n",
        "print(\"\\nFinal Weights and Biases:\")\n",
        "print(\"W1:\", W1)\n",
        "print(\"B1:\", B1)\n",
        "print(\"W2:\", W2)\n",
        "print(\"B2:\", B2)"
      ],
      "metadata": {
        "id": "F_9lDvstKuJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Sepal Width의 변수를 제거하고 모델을 학습시킨 결과"
      ],
      "metadata": {
        "id": "6VoNLoD12HkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['sepal length (cm)', 'petal length (cm)', 'petal width (cm)']].values\n",
        "y = df['species'].values"
      ],
      "metadata": {
        "id": "BdIHWJX31x8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 표준화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "wFbK8pR5-a8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 분할: 훈련 데이터 120개, 테스트 데이터 30개\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5e3t-DEAZTAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 원-핫 인코딩\n",
        "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "Y_train_onehot = encoder.fit_transform(Y_train.reshape(-1, 1))\n",
        "Y_test_onehot = encoder.transform(Y_test.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "94EA2iqsZUtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "inputs = 4  # 입력층 노드 수\n",
        "hiddens = 5  # 은닉층 노드 수\n",
        "outputs = 3  # 출력층 노드 수\n",
        "learning_rate = 0.01  # 학습률\n",
        "epochs = 50  # 최대 학습 에포크 수"
      ],
      "metadata": {
        "id": "nbrrN9LnZWdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 및 바이어스 초기화\n",
        "np.random.seed(42)  # 재현성을 위해 시드 설정\n",
        "W1 = np.random.randn(inputs, hiddens) * 0.01  # 입력층 -> 은닉층 가중치\n",
        "B1 = np.zeros((hiddens,))  # 은닉층 바이어스\n",
        "W2 = np.random.randn(hiddens, outputs) * 0.01  # 은닉층 -> 출력층 가중치\n",
        "B2 = np.zeros((outputs,))  # 출력층 바이어스"
      ],
      "metadata": {
        "id": "ovEALZ6-ZX7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 활성화 함수: sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# sigmoid 함수의 미분\n",
        "def sigmoid_deriv(out):\n",
        "    return out * (1 - out)\n",
        "\n",
        "# 출력층 활성화 함수: softmax\n",
        "def softmax(x):\n",
        "    c = np.max(x, axis=1, keepdims=True)  # 안정적인 softmax 구현\n",
        "    exp_x = np.exp(x - c)\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# 손실 함수: cross-entropy\n",
        "def cross_entropy(pred, target):\n",
        "    return -np.sum(target * np.log(pred + 1e-9)) / len(target)\n",
        "\n",
        "# 모델의 순방향 전파\n",
        "def predict(x):\n",
        "    if len(x.shape) == 1:\n",
        "        x = x.reshape(1, -1)  # 입력 데이터가 1차원인 경우 2차원으로 변환\n",
        "    if x.shape[1] != W1.shape[0]:\n",
        "        raise ValueError(f\"Input shape mismatch: expected {W1.shape[0]}, got {x.shape[1]}\")\n",
        "    layer0 = x  # 입력층 (N, 4)\n",
        "    Z1 = np.dot(layer0, W1) + B1  # 은닉층 선형 연산 (N, 5)\n",
        "    layer1 = sigmoid(Z1)  # 은닉층 활성화 함수 적용 (N, 5)\n",
        "    Z2 = np.dot(layer1, W2) + B2  # 출력층 선형 연산 (N, 3)\n",
        "    layer2 = softmax(Z2)  # 출력층 활성화 함수 적용 (N, 3)\n",
        "    return layer0, layer1, layer2"
      ],
      "metadata": {
        "id": "iLnpaFHHZZZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "def fit(epochs):\n",
        "    global W1, W2, B1, B2\n",
        "\n",
        "    for i in range(epochs):  # 최대 epochs 반복\n",
        "        # 순방향 전파\n",
        "        layer0, layer1, layer2 = predict(X_train)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = cross_entropy(layer2, Y_train_onehot)\n",
        "\n",
        "        # 역전파\n",
        "        layer2_delta = (layer2 - Y_train_onehot) / len(X_train)  # 출력층 gradient\n",
        "        layer1_error = np.dot(layer2_delta, W2.T)  # 은닉층 오차\n",
        "        layer1_delta = layer1_error * sigmoid_deriv(layer1)  # 은닉층 gradient\n",
        "\n",
        "        # 가중치 및 바이어스 업데이트\n",
        "        W2 -= learning_rate * np.dot(layer1.T, layer2_delta)  # 은닉층 -> 출력층 가중치 업데이트\n",
        "        B2 -= learning_rate * np.sum(layer2_delta, axis=0)  # 출력층 바이어스 업데이트\n",
        "        W1 -= learning_rate * np.dot(layer0.T, layer1_delta)  # 입력층 -> 은닉층 가중치 업데이트\n",
        "        B1 -= learning_rate * np.sum(layer1_delta, axis=0)  # 은닉층 바이어스 업데이트\n",
        "\n",
        "        # 학습 상태 출력\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch {i + 1}/{epochs}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "YZeJODRzZbSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "def evaluate(X_data, Y_data):\n",
        "    _, _, layer2 = predict(X_data)\n",
        "    pred_classes = np.argmax(layer2, axis=1)  # 예측 클래스\n",
        "    true_classes = np.argmax(Y_data, axis=1)  # 실제 클래스\n",
        "    accuracy = np.mean(pred_classes == true_classes)  # 정확도 계산\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "-eI5c5-jZdPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 실행\n",
        "fit(epochs=50)\n",
        "\n",
        "# 테스트 데이터 평가\n",
        "accuracy = evaluate(X_test, Y_test_onehot)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# 가중치 및 바이어스 출력\n",
        "print(\"\\nFinal Weights and Biases:\")\n",
        "print(\"W1:\", W1)\n",
        "print(\"B1:\", B1)\n",
        "print(\"W2:\", W2)\n",
        "print(\"B2:\", B2)"
      ],
      "metadata": {
        "id": "o83WgxhIZgf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터의 정렬이 불균형해 문제가 될 것이라고 추측함"
      ],
      "metadata": {
        "id": "TVgoiTkuaUtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 불균형 문제 해결"
      ],
      "metadata": {
        "id": "TpCw6S5FacRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#특성과 타겟 분리\n",
        "X = df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']].values\n",
        "y = df['species'].values"
      ],
      "metadata": {
        "id": "j3dVHrN7Zh6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 표준화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "QDdneK89a8TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 분할: 층화된 분리 적용\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "0XYzdLjKa-A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 분포 확인\n",
        "from collections import Counter\n",
        "print(\"Train class distribution:\", Counter(Y_train))\n",
        "print(\"Test class distribution:\", Counter(Y_test))"
      ],
      "metadata": {
        "id": "C0M40r8YbAdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 원-핫 인코딩\n",
        "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "Y_train_onehot = encoder.fit_transform(Y_train.reshape(-1, 1))\n",
        "Y_test_onehot = encoder.transform(Y_test.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "Tpquk8XIbC5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "inputs = 4  # 입력층 노드 수\n",
        "hiddens = 5  # 은닉층 노드 수\n",
        "outputs = 3  # 출력층 노드 수\n",
        "learning_rate = 0.01  # 학습률\n",
        "epochs = 50  # 최대 학습 에포크 수"
      ],
      "metadata": {
        "id": "EaiIKWZWbKW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 및 바이어스 초기화\n",
        "np.random.seed(42)  # 재현성을 위해 시드 설정\n",
        "W1 = np.random.randn(inputs, hiddens) * 0.01  # 입력층 -> 은닉층 가중치\n",
        "B1 = np.zeros((hiddens,))  # 은닉층 바이어스\n",
        "W2 = np.random.randn(hiddens, outputs) * 0.01  # 은닉층 -> 출력층 가중치\n",
        "B2 = np.zeros((outputs,))  # 출력층 바이어스"
      ],
      "metadata": {
        "id": "gEXqzIGdbMVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 활성화 함수: sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# sigmoid 함수의 미분\n",
        "def sigmoid_deriv(out):\n",
        "    return out * (1 - out)\n",
        "\n",
        "# 출력층 활성화 함수: softmax\n",
        "def softmax(x):\n",
        "    c = np.max(x, axis=1, keepdims=True)  # 안정적인 softmax 구현\n",
        "    exp_x = np.exp(x - c)\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# 손실 함수: cross-entropy\n",
        "def cross_entropy(pred, target):\n",
        "    return -np.sum(target * np.log(pred + 1e-9)) / len(target)"
      ],
      "metadata": {
        "id": "rEV4veWUbN-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 순방향 전파\n",
        "def predict(x):\n",
        "    if len(x.shape) == 1:\n",
        "        x = x.reshape(1, -1)  # 입력 데이터가 1차원인 경우 2차원으로 변환\n",
        "    if x.shape[1] != W1.shape[0]:\n",
        "        raise ValueError(f\"Input shape mismatch: expected {W1.shape[0]}, got {x.shape[1]}\")\n",
        "    layer0 = x  # 입력층 (N, 4)\n",
        "    Z1 = np.dot(layer0, W1) + B1  # 은닉층 선형 연산 (N, 5)\n",
        "    layer1 = sigmoid(Z1)  # 은닉층 활성화 함수 적용 (N, 5)\n",
        "    Z2 = np.dot(layer1, W2) + B2  # 출력층 선형 연산 (N, 3)\n",
        "    layer2 = softmax(Z2)  # 출력층 활성화 함수 적용 (N, 3)\n",
        "    return layer0, layer1, layer2"
      ],
      "metadata": {
        "id": "KIpPDrw_bPte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "def fit(epochs):\n",
        "    global W1, W2, B1, B2\n",
        "\n",
        "    for i in range(epochs):  # 최대 epochs 반복\n",
        "        # 순방향 전파\n",
        "        layer0, layer1, layer2 = predict(X_train)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = cross_entropy(layer2, Y_train_onehot)\n",
        "\n",
        "        # 역전파\n",
        "        layer2_delta = (layer2 - Y_train_onehot) / len(X_train)  # 출력층 gradient\n",
        "        layer1_error = np.dot(layer2_delta, W2.T)  # 은닉층 오차\n",
        "        layer1_delta = layer1_error * sigmoid_deriv(layer1)  # 은닉층 gradient\n",
        "\n",
        "        # 가중치 및 바이어스 업데이트\n",
        "        W2 -= learning_rate * np.dot(layer1.T, layer2_delta)  # 은닉층 -> 출력층 가중치 업데이트\n",
        "        B2 -= learning_rate * np.sum(layer2_delta, axis=0)  # 출력층 바이어스 업데이트\n",
        "        W1 -= learning_rate * np.dot(layer0.T, layer1_delta)  # 입력층 -> 은닉층 가중치 업데이트\n",
        "        B1 -= learning_rate * np.sum(layer1_delta, axis=0)  # 은닉층 바이어스 업데이트\n",
        "\n",
        "        # 학습 상태 출력\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch {i + 1}/{epochs}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "0KeHcdMVbR7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "def evaluate(X_data, Y_data):\n",
        "    _, _, layer2 = predict(X_data)\n",
        "    pred_classes = np.argmax(layer2, axis=1)  # 예측 클래스\n",
        "    true_classes = np.argmax(Y_data, axis=1)  # 실제 클래스\n",
        "    accuracy = np.mean(pred_classes == true_classes)  # 정확도 계산\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "XcTlqpAebT6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 실행\n",
        "fit(epochs=50)\n",
        "\n",
        "# 테스트 데이터 평가\n",
        "accuracy = evaluate(X_test, Y_test_onehot)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# 가중치 및 바이어스 출력\n",
        "print(\"\\nFinal Weights and Biases:\")\n",
        "print(\"W1:\", W1)\n",
        "print(\"B1:\", B1)\n",
        "print(\"W2:\", W2)\n",
        "print(\"B2:\", B2)"
      ],
      "metadata": {
        "id": "o6dmOrcIbVsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아까전보다 0.33%의 정확도가 늘어났다."
      ],
      "metadata": {
        "id": "1CHvI8BmbY_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#파이토치를 활용하여 성능 강화"
      ],
      "metadata": {
        "id": "S5lvTYp0M5kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "tjUSCaA7b2aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#특성과 타겟 분리\n",
        "X = df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']].values\n",
        "y = df['species'].values"
      ],
      "metadata": {
        "id": "P6FyzbjzNcrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 표준화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "yFhUH6HpSaWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "1UE0SWA-SfNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 분포 확인\n",
        "from collections import Counter\n",
        "print(\"Train class distribution:\", Counter(Y_train))\n",
        "print(\"Test class distribution:\", Counter(Y_test))"
      ],
      "metadata": {
        "id": "5wRVE8zZSq-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 원-핫 인코딩\n",
        "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "Y_train_onehot = encoder.fit_transform(Y_train.reshape(-1, 1))\n",
        "Y_test_onehot = encoder.transform(Y_test.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "HV1IwtHzSuXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "inputs = 4  # 입력층 노드 수\n",
        "hiddens = 5  # 은닉층 노드 수\n",
        "outputs = 3  # 출력층 노드 수\n",
        "learning_rate = 0.01  # 학습률\n",
        "epochs = 50  # 최대 학습 에포크 수"
      ],
      "metadata": {
        "id": "KbjdMkheSwtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']].values\n",
        "y = df['species'].values\n",
        "\n",
        "# 데이터 전처리: 표준화\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# 훈련 데이터와 테스트 데이터 분할 (8:2 비율)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle = True, stratify = y)\n",
        "# 계층적 샘플링 넣음 stratify=y\n",
        "\n",
        "# PyTorch 텐서로 변환\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# 신경망 모델 정의\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_dim, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Define fc1 layer\n",
        "        self.relu = nn.ReLU()                        # Define ReLU activation\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim) # Define fc2 layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 모델, 손실 함수, 최적화 알고리즘 설정\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 64\n",
        "output_dim = len(torch.unique(y_train_tensor))  # use torch.unique to find unique elements\n",
        "\n",
        "model = MLP(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 학습\n",
        "epochs = 50\n",
        "batch_size = 16\n",
        "\n",
        "for epoch in range(epochs):  # Iterate over epochs\n",
        "    for i in range(0, len(X_train_tensor), batch_size):  # Iterate over batches\n",
        "        batch_X = X_train_tensor[i:i + batch_size]\n",
        "        batch_y = y_train_tensor[i:i + batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            train_outputs = model(X_train_tensor)\n",
        "            train_predictions = torch.argmax(train_outputs, dim=1)\n",
        "            train_accuracy = (train_predictions == y_train_tensor).sum().item() / len(y_train_tensor)\n",
        "\n",
        "            test_outputs = model(X_test_tensor)\n",
        "            test_predictions = torch.argmax(test_outputs, dim=1)\n",
        "            test_accuracy = (test_predictions == y_test_tensor).sum().item() / len(y_test_tensor)\n",
        "\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {loss.item():.4f}, \"\n",
        "                  f\"Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# 테스트 데이터에 대한 예측 및 정확도 계산\n",
        "with torch.no_grad():\n",
        "    predictions = model(X_test_tensor) # Change X_test to X_test_tensor\n",
        "    _, predicted_labels = torch.max(predictions, 1)  # 가장 높은 확률을 가진 클래스 선택\n",
        "    accuracy = (predicted_labels == y_test_tensor).sum().item() / len(y_test_tensor) # Change y_test to y_test_tensor\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# 학습된 가중치와 편향 출력\n",
        "print(\"\\nHidden Weights:\\n\", model.fc1.weight)\n",
        "print(\"Hidden Bias:\\n\", model.fc1.bias)\n",
        "print(\"Output Weights:\\n\", model.fc2.weight)\n",
        "print(\"Output Bias:\\n\", model.fc2.bias)"
      ],
      "metadata": {
        "id": "wqktJ72MSzA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: 0.9667"
      ],
      "metadata": {
        "id": "I63mAnB7bvT2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vl1FpbyzUAk1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}